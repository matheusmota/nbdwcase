{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs 1, 2, 3 - Building the Trusted Layer \n",
    "\n",
    "This notebook describes the rationale behind the jobs. All three jobs are already integrated in NiFi and are automatically triggered.  (http://localhost:8080/nifi/) \n",
    "\n",
    "<p style=\"color:red\"><b>This notebook is not the job it self. You do not need to run it.</b></p> \n",
    "\n",
    "Instead, it just documents my rationale while job were designed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, socket\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "conf.set(\"spark.executor.memory\", \"20g\")\n",
    "conf.set(\"spark.driver.memory\", \"20g\")\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minioadmin\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minioadmin\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://{}:9000\".format(socket.gethostbyname_ex('minio-s3')[2][0]))\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "sqlc = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import json\n",
    "\n",
    "\n",
    "def describe_dataframe(df):\n",
    "    \"\"\"Describe a given dataframe by printing key metrics.\n",
    "\n",
    "    Metrics:\n",
    "    - number_of_rows\n",
    "    - number_of_unique_rows\n",
    "    - number_of_duplicated_rows (%)\n",
    "    \n",
    "    \"\"\"    \n",
    "    number_of_rows = df.count()\n",
    "    number_of_unique_rows = df.dropDuplicates().count()\n",
    "    number_of_duplicated_rows = number_of_rows - number_of_unique_rows\n",
    "    \n",
    "    percent_of_unique_rows = number_of_unique_rows/number_of_rows * 100\n",
    "    percent_of_duplicated_rows = number_of_duplicated_rows/number_of_rows * 100\n",
    "    \n",
    "    print(\"Total      rows: {}\".format(number_of_rows))\n",
    "    print(\"Unique     rows: {} ({:.2f}%)\".format(number_of_unique_rows,percent_of_unique_rows))\n",
    "    print(\"Duplicated rows: {} ({:.2f}%)\".format(number_of_duplicated_rows, percent_of_duplicated_rows))\n",
    "\n",
    "    \n",
    "def add_prefix_to_df_columns(df, prefix=\"\", separator=\"_\", escape=[]):\n",
    "    \"\"\"\n",
    "    Adds prefix to a DF ignoring current prefixes\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if (c not in escape) and (prefix not in c) :\n",
    "            df = df.withColumnRenamed(c, '{}{}{}'.format(prefix, separator, c))\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_json(record):\n",
    "    \"\"\"\n",
    "    Fixing common problems found in JSON lint\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        updated_record = record.replace(r'\\\", \"', '\\ \", \"')\n",
    "        updated_record = updated_record.replace(r'\\\"}', '\\ \"}')      \n",
    "        updated_record = updated_record.replace(r'\\\"', '\\\"')      \n",
    "      \n",
    "        json.loads(updated_record)   # trying to parse...\n",
    "        return '{ \"data\" : '+ updated_record +' }'\n",
    "        \n",
    "    except Exception as e:\n",
    "        \n",
    "        return 'invalid'\n",
    "    \n",
    "parse_and_fix_json = F.udf(validate_json, T.StringType())\n",
    "\n",
    "def anonimize_columns(df, target_columns=[], prefix=None, numBits=256):\n",
    "\n",
    "    \"\"\"\n",
    "    Applies a SHA-2 family of hash functions to columns (replace)\n",
    "    \"\"\"    \n",
    "    \n",
    "    for column in target_columns:\n",
    "        if(prefix):\n",
    "            df = df.withColumn(column, F.sha2(F.concat(prefix, \"_\", F.col(column).cast(\"string\")), numBits))\n",
    "        else:\n",
    "            df = df.withColumn(column, F.sha2(F.col(column).cast(\"string\"), numBits))\n",
    "    return df\n",
    "\n",
    "def compare_dfs(key_dfs=[]):\n",
    "\n",
    "    \"\"\"\n",
    "    Compare key dfs distinct values in order to validade possible missing data\n",
    "    \"\"\"    \n",
    "      \n",
    "    counts = []\n",
    "    \n",
    "    for df in key_dfs:       \n",
    "        counts.append(df.dropDuplicates().count())\n",
    "    \n",
    "    if not (all(x == counts[0] for x in counts)):\n",
    "        raise Exception('Dataframes do not match count. Please check the pipe. Aborting. {}'.format(counts))\n",
    "    \n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading main dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant = sqlc.read.parquet('s3a://raw-data/restaurant.csv.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_restaurant.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status = sqlc.read.parquet('s3a://raw-data/status.json.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumer = sqlc.read.csv('s3a://raw-data/consumer.csv', header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_consumer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = sqlc.read.parquet('s3a://raw-data/order.json.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Sanitization & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More work on this matter should be investigated. We would probaly need more generic solutions on this problem; maybe an internal tool, specially considering business scenarios. In this solution only critical columns will be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling schema-related issues: fixing timestamp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status = df_status.withColumn('created_at',F.unix_timestamp(F.lit(df_status.created_at),\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status = df_status.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    display(df_status.select(\"value\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    print(df_status.groupBy(\"order_id\").count().filter(\"count != 3\").count())\n",
    "    df_status.groupBy(\"order_id\").count().filter(\"count != 3\").sort(F.desc(\"count\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the same status appears multiple times. Checking status counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status.groupBy(\"order_id\", \"value\").count().filter(\"count > 1 AND value == 'REGISTERED' \").sort(F.desc(\"count\")).show(5, False)\n",
    "    df_status.groupBy(\"order_id\", \"value\").count().filter(\"count > 1 AND value == 'CONCLUDED' \").sort(F.desc(\"count\")).show(10, False)\n",
    "    df_status.groupBy(\"order_id\", \"value\").count().filter(\"count > 1 AND value == 'PLACED' \").sort(F.desc(\"count\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status.filter(\"order_id == '1207262a-e90d-40eb-8714-080490acc201' \").sort(\"created_at\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_agg_by_order_and_value =  df_status.groupBy(\"order_id\",\"value\").agg(F.count(\"value\").alias(\"count\"), F.collect_list(\"created_at\").alias(\"created_at_array\"))\\\n",
    "                                             .withColumn(\"min_created_at\", F.coalesce(F.array_min(\"created_at_array\")))\\\n",
    "                                             .withColumn(\"max_created_at\", F.coalesce(F.array_max(\"created_at_array\")))\\\n",
    "                                             .withColumn(\"diff_created_at\", F.unix_timestamp(\"max_created_at\") - F.unix_timestamp(\"min_created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status_agg_by_order_and_value.filter(\"diff_created_at > 0\").orderBy(*[\"order_id\"], ascending=False).show(10, False)\n",
    "    df_status_agg_by_order_and_value.groupBy(\"order_id\").agg(F.count(\"order_id\").alias(\"count\")).filter(\"count !=3\").sort(F.desc(\"count\")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status.filter(\"order_id == 'adefb27b-7ea7-46a8-b0ef-41f2f868b0b9'\").sort(\"created_at\").show(5,False)\n",
    "    df_status.filter(\"order_id == 'a92c7759-5f08-419b-877c-d75842481e69'\").sort(\"created_at\").show(5,False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. I will have to check if it is a parsing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    sqlc.read.json('s3a://raw-data/status.json').filter(\" order_id == '01d83b11-450a-4917-b2d8-2121b1f3f5ef'\").sort(\"created_at\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odd. No business information available, so I have to make some decisions. \n",
    "\n",
    "I decided to consider the max(timestamp). Requeriments indicate LAST as more useful information.\n",
    "\n",
    "Just in case, lets check if all orders have at least one \"PLACED\" status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    print(df_status.select(\"order_id\").distinct().count())\n",
    "    print(df_status.select(\"order_id\", \"value\").filter(\"value == 'PLACED'\").distinct().count())\n",
    "    print(df_status.select(\"order_id\", \"value\").filter(\"value == 'REGISTERED'\").distinct().count())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business-related transformations (requirements)\n",
    "##### Calculating the last status for each order\n",
    "\n",
    "Requirement: \"add the the LAST status from order statuses dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_agg_by_order = df_status.sort(\"created_at\").groupBy(\"order_id\").agg(F.count(\"value\").alias(\"count_value\"), F.collect_list(\"created_at\").alias(\"created_at_array\"), F.collect_list(\"value\").alias(\"value_array\"), )\\\n",
    "                                  .withColumn(\"first_created_at\", F.coalesce(F.array_min(\"created_at_array\")))\\\n",
    "                                  .withColumn(\"last_created_at\",  F.coalesce(F.array_max(\"created_at_array\")))\\\n",
    "                                  .withColumn(\"diff_last_and_first_created_at\", F.unix_timestamp(\"last_created_at\") - F.unix_timestamp(\"first_created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status_agg_by_order.filter(\"diff_last_and_first_created_at > 0 AND count_value != 3\").orderBy(*[\"order_id\"], ascending=False).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_agg_by_order = df_status_agg_by_order.withColumnRenamed(\"last_created_at\",\"created_at\").join(df_status,['order_id','created_at']).withColumnRenamed(\"created_at\",\"last_created_at\").withColumnRenamed(\"value\",\"last_value\").withColumnRenamed(\"status_id\",\"last_status_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status_agg_by_order.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_status_agg_by_order.select(\"order_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling schema-related issues: fixing timestamp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = df_order.withColumn('order_created_at',F.unix_timestamp(F.lit(df_order.order_created_at),\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling schema-related issues: JSON parsing problems\n",
    "\n",
    "\n",
    "A User Defined Function was built to address formatting problems. The UDF was moved to the utils package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = df_order.withColumn(\"items_json_text\", parse_and_fix_json(F.col(\"items\")))\n",
    "\n",
    "invalid_json_items_count = df_order.filter(df_order.items_json_text == 'invalid').count()\n",
    "\n",
    "if(DEBUG):\n",
    "    print(\"Total of invalid json items (df_order): {}\".format(invalid_json_items_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infering the schema for *items*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_json_schema = sqlc.read.json(df_order.rdd.map(lambda row: row.items_json_text)).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = df_order.withColumn('items', F.from_json(F.col(\"items_json_text\"), items_json_schema)).drop(\"items_json_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = df_order.withColumn(\"items\", F.col(\"items\").data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark about previous schema building: we could improve this infering process by taking a sample of data. Further evaluation/heuristics of a proper strategy will be needed and therefore will not be addressed here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal elements of `items` would need to be better 'typed' (e.g., there are some fields being treated as string but should be correctly 'typed' as numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time restriction, other columns are being neglected here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anonymizing sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: move to https://pypi.org/project/spark-privacy-preserver/\n",
    "\n",
    "# Further information can be anonimized here\n",
    "sensitive_columns = [\"cpf\", \"customer_name\", \"delivery_address_latitude\", \"delivery_address_longitude\", \"delivery_address_zip_code\"]\n",
    "df_order = anonimize_columns(df_order, target_columns=sensitive_columns, numBits=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_order.select(\"order_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we have a problem. Something is duplicated. I took a sample and noticed `order_created_at` and `cpf` causing duplicates.\n",
    "Confirming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_order.drop(\"cpf\").dropDuplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_order.drop(\"order_created_at\").dropDuplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_order.drop(*[\"cpf\",\"order_created_at\"]).dropDuplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_order.drop(*[\"cpf\",\"order_created_at\"]).dropDuplicates().select(\"order_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, lets build the final df for orders without losing the information causing duplicated orders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_agg_by_order_id = df_order.groupBy(\"order_id\").agg(F.collect_list(\"order_created_at\").alias(\"order_created_at_array\"), F.collect_list(\"cpf\").alias(\"cpf_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    display(df_order_agg_by_order_id.limit(3))\n",
    "    describe_dataframe(df_order_agg_by_order_id.select(\"order_id\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging will all other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_agg_by_order_id = df_order_agg_by_order_id.join(df_order.drop(*[\"cpf\",\"order_created_at\"]).dropDuplicates(), \"order_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    display(df_order_agg_by_order_id.limit(3))\n",
    "    describe_dataframe(df_order_agg_by_order_id.select(\"order_id\"))    \n",
    "    df_order_agg_by_order_id.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order_agg_by_order_id = df_order_agg_by_order_id.drop(*[\"customer_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anonymizing sensitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Do: move to https://pypi.org/project/spark-privacy-preserver/\n",
    "\n",
    "# Further information can be anonimized here\n",
    "sensitive_columns = [\"customer_name\", \"customer_phone_number\"]\n",
    "df_consumer = anonimize_columns(df_consumer, target_columns=sensitive_columns, numBits=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building trusted dataset (requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we are ready to meet all the requirements for the datamart.\n",
    "\n",
    "First, lets **avoid column key crashes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_restaurant.printSchema()\n",
    "\n",
    "df_restaurant = add_prefix_to_df_columns(df_restaurant,\"merchant\", escape=[])\n",
    "\n",
    "if(DEBUG):\n",
    "    df_restaurant.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_consumer.printSchema()\n",
    "\n",
    "df_consumer   = add_prefix_to_df_columns(df_consumer,\"customer\", escape=[])\n",
    "\n",
    "if(DEBUG):\n",
    "    df_consumer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status_agg_by_order.printSchema()\n",
    "\n",
    "df_status_agg_by_order = add_prefix_to_df_columns(df_status_agg_by_order,\"status\", escape=['order_id'])  \n",
    "\n",
    "if(DEBUG):\n",
    "    df_status_agg_by_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining order with consumer, restaurant, and status\n",
    "\n",
    "Requirement: *one line per order with all data from order, consumer, restaurant*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order = df_order_agg_by_order_id.join(df_consumer, 'customer_id', \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order = df_trusted_order.join(df_restaurant, 'merchant_id', \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order = df_trusted_order.join(df_status_agg_by_order, 'order_id', \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    display(df_trusted_order.limit(3))\n",
    "    describe_dataframe(df_trusted_order.select(\"order_id\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding LOCAL datetime (will be used for partitioning, as required)\n",
    "\n",
    "Requirement: *To help analysis, it would be a nice to have: data partitioned on the restaurant LOCAL date.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order = df_trusted_order.withColumn(\"localtime_order_created_at\",pyspark.sql.functions.to_utc_timestamp(F.col(\"status_last_created_at\"), F.col(\"merchant_timezone\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order = df_trusted_order.withColumn(\"localtime_order_created_at_date\", F.to_date(F.col(\"localtime_order_created_at\") ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics and validation\n",
    "\n",
    "As a simple validation, I will check if all ids (customers, merchants, orders, order_id from df_status are in the same final dataframe). I validate this aspect by a simple count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[809323, 809323]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_trusted_order = df_order_agg_by_order_id.join(df_consumer, 'customer_id')\n",
    "\n",
    "key_dfs=[df_trusted_order.select(\"customer_id\").where(F.col(\"customer_id\").isNotNull()), \n",
    "         df_consumer.select(\"customer_id\")\n",
    "]\n",
    "compare_dfs(key_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7292, 7292]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_trusted_order = df_trusted_order.join(df_restaurant, 'merchant_id')\n",
    "\n",
    "key_dfs=[df_trusted_order.select(\"merchant_id\").where(F.col(\"merchant_id\").isNotNull()), \n",
    "         df_restaurant.select(\"merchant_id\")\n",
    "]\n",
    "compare_dfs(key_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-f91ccc138505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0mdf_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcompare_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-86216ee6fa9b>\u001b[0m in \u001b[0;36mcompare_dfs\u001b[0;34m(key_dfs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_dfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_trusted_order = df_trusted_order.join(df_order, 'order_id')\n",
    "\n",
    "key_dfs=[df_trusted_order.select(\"order_id\").where(F.col(\"order_id\").isNotNull()), \n",
    "         df_order.select(\"order_id\")\n",
    "]\n",
    "compare_dfs(key_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datamart - Storing order-dataset in trusted layer partioned by LOCAL date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order.repartition(\"localtime_order_created_at_date\").write.partitionBy(\"localtime_order_created_at_date\").mode(\"overwrite\").format(\"parquet\").save(\"s3a://trusted-data/order-dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Items Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding items externalIds and linking to order_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_trusted_layer = sqlc.read.parquet(\"s3a://trusted-data/order-dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order_items = df_orders_trusted_layer.select('order_id','items').withColumn('items', F.explode('items'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_trusted_order_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order_items = df_trusted_order_items.select(\"order_id\", \"items.name\", \"items.externalId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_trusted_order_items.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order_items = df_trusted_order_items.groupBy(\"externalId\").agg(F.collect_list(\"order_id\").alias(\"order_ids\"), F.collect_set(\"name\").alias(\"item_names\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datamart - Storing order_items dataset in trusted layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trusted_order_items.repartition(1).write.format(\"parquet\").mode(\"append\").save(\"s3a://trusted-data/order_items-dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Statuses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_trusted_layer = sqlc.read.parquet(\"s3a://trusted-data/order-dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_raw = sqlc.read.parquet('s3a://raw-data/status.json.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivoting on values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_raw = df_status_raw.groupBy('order_id').pivot(\"value\").agg(F.collect_set(\"created_at\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    df_status_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_orders_trusted_layer.select(\"order_id\"))\n",
    "    describe_dataframe(df_status_raw.select(\"order_id\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep. We should merge with order dataset to make sure all ids are there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with missing ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_raw = df_status_raw.join(df_orders_trusted_layer.select(\"order_id\"), 'order_id', \"right_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(DEBUG):\n",
    "    describe_dataframe(df_orders_trusted_layer.select(\"order_id\"))\n",
    "    describe_dataframe(df_status_raw.select(\"order_id\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating datamart - Storing order_items dataset in trusted layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_raw.repartition(1).write.format(\"parquet\").mode(\"append\").save(\"s3a://trusted-data/order_statuses-dataset.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
